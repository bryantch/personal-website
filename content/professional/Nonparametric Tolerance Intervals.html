

<p><strong>Introduction</strong></p>
<p>This post is going to be about a method of constructing Nonparametric
Tolerance Intervals with a focus on it’s derivation, simulation and
application to real data. In particular, Wilk’s method for determining
the limits of the interval based on order statistics, is covered here at
some mathematical depth.</p>
<p>First, what is a tolerance interval? A tolerance interval is different
from the familiar confidence interval and even a prediction interval in
that a tolerance interval provides limits in which at least a certain
proportion of the sampled population falls within with a specified level
of confidence. Tolerance intervals are lesser known compared to the
other two but are widely utilized in manufacturing environments. Whereas
confidence and prediction intervals are meant to bound a population
parameter and a future sampled value, respectively, tolerance intervals
bound a range of values that make up a certain proportion of the
population.</p>
<p>To correctly interpret a computed tolerance interval, say for one that
covers 90% of the population with 95% confidence - the proportion of
tolerance intervals which contain at least 90% of the population
computed from samples obtained from repeated sampling of the population
is 95%. Similarly, there is a 95% chance that the 90/95 tolerance
interval calculated from a given future sample will bound at least 90%
of the population.</p>
<p>For this introductory post, an emphasis is placed on constructing a
tolerance interval but much like one-sided upper or lower confidence and
prediction limits, one can also devise one-sided upper or lower
tolerance limits, both of which that have the most desired property of
having their nominal coverage probability hold. This means that on
repeated sampling of the population, yielding independent data sets and
a set of tolerance intervals computed from these then the fraction of
these computed intervals that include at least the specified proportion
of the population, equal the specified confidence level or nominal
coverage probability associated with the interval. Any mismatch between
the actual coverage probability and the nominal coverage probability
typically occurs when there are issues with distributional assumptions
as well as in cases of approximating a discrete distribution with a
continuous one like methods for constructing binomial confidence
intervals based on the normal distribution.</p>
<p>Nonparametric methods are attractive when the distributional assumptions
cannot be relied upon with parametric methods. There are several
techniques for constructing a nonparametric tolerance interval but as
mentioned aboved, Wilk’s method will be the one that is used for doing
so and evaluated here for having the nominal coverage probability.</p>
<p><strong>Derivation</strong></p>
<p>Suppose we have a sequence of random variables that follow some
probability distribution -</p>
<p><span class="math display">\[X_1,X_2,...,X_n \sim f(x)\]</span> where <span class="math inline">\(f(x)\)</span> is unknown and -
<span class="math display">\[X_{(1)},X_{(2)},...,X_{(n)}\]</span> are the order statistics while
<span class="math inline">\(x_{(1)},x_{(2)},...,x_{(n)}\)</span> is the ordered sample.</p>
<p>For a two-sided tolerance interval, we want an interval for some
population, <span class="math inline">\(p\)</span> and confidence level, <span class="math inline">\(1-a\)</span>, such that -</p>
<p><span class="math display">\[P \left[ \Bigl\{ P[X \leq LL] \leq \frac{(1-p)}{2} \Bigr\} \cap \Bigl\{P[X \geq UL] \leq \frac{(1-p)}{2} \Bigr\} \right]= 1-a \]</span>
We define <span class="math inline">\(LL\)</span> and <span class="math inline">\(UL\)</span> to be the values of the order statistics that
satisfies <span class="math inline">\((F_X(UL) - F_X(LL)) \geq p\)</span>. So that means we need to find
ranks, <span class="math inline">\(d\)</span> and <span class="math inline">\(c\)</span>, such that <span class="math inline">\(F(X_{(d)}) - F(X_{(c)}) \geq p\)</span> which is
equivalent to the following -</p>
<p><span class="math display">\[F(X_{(d)}) - F(X_{(c)}) \geq p \leftrightarrow \left[1 - \int_{X_{(c)}}^{X_{(d)}} f(x) dx \geq p \right]\]</span>
where <span class="math inline">\(F\)</span> is the cumulative distribution function of the unknown
population. We set <span class="math inline">\(W = F(X_{(d)}) - F(X_{(c)})\)</span> so <span class="math inline">\(W\)</span> is a random
variable.</p>
<p>It can be shown that the <span class="math inline">\(kth\)</span> smallest order statistic follows a Beta
Distribution, <span class="math inline">\(Beta(\alpha,\beta)\)</span>, with <span class="math inline">\(\alpha = k\)</span> and
<span class="math inline">\(\beta = n - k + 1\)</span>.</p>
<p><span class="math display">\[X_{(k)} \sim Beta(k,n-k+1)\]</span> <span class="math display">\[c = r\]</span> <span class="math display">\[d = n-r+1\]</span>
<span class="math display">\[d-c = n - r+1-r = n-2r+1 = a\]</span> <span class="math display">\[n-(n-2r+1)+1 = n-n+2r-1+1 = 2r = b\]</span>
<span class="math display">\[\rightarrow X_{(d-c)} \sim Beta(a = n-2r+1, b = 2r)\]</span> So since <span class="math inline">\(W\)</span> is
a function of the range between the order statistics with ranks <span class="math inline">\(d\)</span> and
<span class="math inline">\(c\)</span>, we can say <span class="math inline">\(W \sim Beta(d-c,n-d+c+1)\)</span>.</p>
<p><span class="math display">\[\rightarrow P[W \geq p] = 1-\alpha \leftrightarrow P[W \leq p] = \alpha\]</span>
<span class="math display">\[\leftrightarrow P[Beta(d - c,n-d + c +1) \leq p] = \alpha\]</span> Note: No
distribution about the population is assumed but the Beta distribution
used here, arises due to being able to characterize an order statistic’s
distribution with the Beta distribution so it is independent of the
population’s distribution. Order statistics are central to nonparametric
methods and are heavily relied upon.</p>
<p>We can simplify this a bit by assuming symmetry of the ranks from their
endpoints. For example, if <span class="math inline">\(n = 65\)</span> then <span class="math inline">\(x_{(2)}\)</span> and
<span class="math inline">\(x_{(64)},x_{(3)},...,x_{(k)}\)</span> and <span class="math inline">\(x_{(n-k+1)}\)</span> are symmetric with each
other in ranks.</p>
<p>To do this, we take <span class="math inline">\(c = r\)</span> and <span class="math inline">\(d = n-r+1\)</span> so that we end up with -</p>
<p><span class="math display">\[P[Beta(d-c,n-d+c+1) \leq p] = \alpha \rightarrow P[Beta(n-2r+1,2r) \leq p] = \alpha\]</span>
From here, we simply use the cumulative distribution function (CDF) of
the Beta distribution which is the regularized incomplete beta function,
<span class="math inline">\(I_x(a,b)\)</span>, to find the ranks of our sorted data that ensures the CDF at
<span class="math inline">\(p\)</span> evaluates to <span class="math inline">\(\alpha\)</span> so that we can take the complement of this
which would give us our confidence level, <span class="math inline">\(1-\alpha\)</span>.</p>
<p>The regularized incomplete beta function, evaluated at the ranks is -</p>
<p><span class="math display">\[I_p(n-2r+1,2r) = \frac{\int_0^{p} t^{n-2r+1-1}(1-t)^{2r-1}dt}{\int_0^{1} t^{n-2r+1-1}(1-t)^{2r-1}dt}\]</span></p>
<p>If <span class="math inline">\(r = 1\)</span> then this simplifies to -</p>
<p><span class="math display">\[\frac{\int_0^{p} t^{n-2r+1-1}(1-t)^{2r-1}dt}{\int_0^{1} t^{n-2r+1-1}(1-t)^{2r-1}dt} = \frac{\frac{np^{(n-1)} + p^n(n-1)}{(n-1)n}}{\frac{1}{(n-1)n}} = np^{(n-1)}+p^n(n-1)\]</span>
<span class="math display">\[\leftrightarrow 1-np^{(n-1)} + p^n(n-1) = 1-\alpha\]</span> If <span class="math inline">\(r \ne 1\)</span>,
meaning we do not want to use the smallest and largest sample values for
coverage but instead the 2nd smallest and largest or 3rd smallest and
largest, etc. then this is the generalized case for that. It is
analytically difficult to solve so numerical methods are most likely
required. That said, this is not required if we intend to use the
smallest and largest values of our sample for bounding the population no
matter how large our sample size is. However, by closely examining this
expression, we can see that more samples would be required to bound the
population using higher order ranks in order to achieve the same level
of confidence using just the minimum and maximum of the sample.</p>
<p><span class="math display">\[1 - \frac{\int_0^p t^{n-2r}(1-t)^{2r-1}dt}{\int_0^1 t^{n-2r}(1-t)^{2r-1}dt} = 1 - \alpha\]</span>
This method has been verified via simulation with various mockup
populations to demonstrate that, indeed, <span class="math inline">\(1-\alpha %\)</span> of intervals
computed under this method contains at least the specified proportion of
the population, regardless of the population’s distribution which we
showcase in the next section.</p>
<p><em>Sample Size Formulation</em></p>
<p>We now have a sample size formula to determine the number of samples to
gather that will bound the population within at least a proportion, <span class="math inline">\(p\)</span>,
with a level of confidence, <span class="math inline">\(1-\alpha\)</span> when using the smallest and
largest sample values. We simply iterate through <span class="math inline">\(n\)</span> for a desired
proportion coverage until the level of confidence is achieved.</p>
<p><span class="math display">\[1 - np^{(n-1)} + p^n(n-1) = 1 - \alpha\]</span></p>
<p>To visualize the relationship between these variables via the formula above - sample size
<span class="math inline">\((n)\)</span>, population coverage <span class="math inline">\((p)\)</span> and confidence level <span class="math inline">\(1-/alpha\)</span>, they are plotted together below.</p>
<pre class="r"><code># Number of samples vs. confidence lvl vs. population coverage
ss = c(seq(40,120,by = 10))
pc = c(seq(.80,.99,by = .03))
cl = c()

cl = matrix(data = NA, nrow = length(ss),ncol = length(pc))
for(i in 1:length(ss)){
  for (j in 1:length(pc)){
    cl[i,j] = 1-ss[i]*pc[j]^(ss[i]-1) + (pc[j]^ss[i])*(ss[i]-1)
  }
}

row.names(cl) = ss
colnames(cl) = pc
cl</code></pre>
<pre><code>##           0.8      0.83      0.86      0.89      0.92      0.95      0.98
## 40  0.9985378 0.9946719 0.9819834 0.9438091 0.8405507 0.6009359 0.1904625
## 50  0.9998073 0.9989891 0.9951488 0.9788353 0.9172880 0.7205682 0.2642286
## 60  0.9999755 0.9998146 0.9987352 0.9922644 0.9582287 0.8084466 0.3380961
## 70  0.9999970 0.9999668 0.9996778 0.9972337 0.9793173 0.8707922 0.4095594
## 80  0.9999996 0.9999942 0.9999193 0.9990270 0.9899133 0.9139462 0.4770265
## 90  1.0000000 0.9999990 0.9999801 0.9996621 0.9951396 0.9432720 0.5395680
## 100 1.0000000 0.9999998 0.9999951 0.9998839 0.9976807 0.9629188 0.5967283
## 110 1.0000000 1.0000000 0.9999988 0.9999605 0.9989022 0.9759324 0.6483834
## 120 1.0000000 1.0000000 0.9999997 0.9999866 0.9994839 0.9844728 0.6946347</code></pre>
<pre class="r"><code>library(lattice)
wireframe(cl,
          zlab = &#39;1-a&#39;,
          xlab = &#39;n&#39;,
          ylab = &#39;p&#39;, scales = list(arrows = FALSE),
          drape = TRUE, colorkey = TRUE, main = &quot;Confidence Level vs. Population Coverage vs. Sample Size&quot;)</code></pre>
<p><img src="/professional/Nonparametric%20Tolerance%20Intervals_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p><strong>Simulation</strong></p>
<p>To see that this method works, we’ll perform a simulation where we
create a fake population that is randomly sampled from. Using the sample
from the population, we’ll construct a tolerance interval using Wilk’s
method that we’ve covered above by utilizing the tolerance package which
can conveniently compute one based on a method specified from several
supported by it. As these limits have the probability of bounding the
population with at least the proportion of interest, we’ll also assess
how frequently this is the case to verify that the actual coverage
equates to the nominal coverage as desired. The code for the simulation
and for evaluating the coverage probability is as follows -</p>
<pre class="r"><code>library(tolerance)</code></pre>
<pre><code>## tolerance package, version 3.0.0, Released 2024-04-18
## This package is based upon work supported by the Chan Zuckerberg Initiative: Essential Open Source Software for Science (Grant No. 2020-255193).</code></pre>
<pre class="r"><code>population = c(rnorm(500,mean = 25,sd = 2),rlnorm(500,meanlog = 3,sdlog = sqrt(.25))) # fake population
hist(population, main = &quot;Histogram of Fake Population&quot;,xlab = &quot;&quot;)</code></pre>
<p><img src="/professional/Nonparametric%20Tolerance%20Intervals_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<pre class="r"><code>our_sample = sample(x = population,size = 65,replace = FALSE)
hist(our_sample, main = &quot;Histogram of Sample Obtained from Fake Population&quot;, xlab = &quot;&quot;)</code></pre>
<p><img src="/professional/Nonparametric%20Tolerance%20Intervals_files/figure-html/unnamed-chunk-2-2.png" width="672" /></p>
<pre class="r"><code>tolerance::nptol.int(our_sample,alpha = .10,P = .90,side = 2,method = &quot;WILKS&quot;) # 90/90 nonparametric tolerance interval based on Wilk&#39;s method</code></pre>
<pre><code>##   alpha   P 2-sided.lower 2-sided.upper
## 1   0.1 0.9       9.65276      39.69055</code></pre>
<pre class="r"><code>tolerance::normtol.int(our_sample, alpha = .10,P = .10,side = 2,method = &quot;HE&quot;) # 90/90 parametric tolerance interval based on normal distribution</code></pre>
<pre><code>##   alpha   P    x.bar 2-sided.lower 2-sided.upper
## 1   0.1 0.1 23.81177      22.68655      24.93698</code></pre>
<pre class="r"><code>n = 1000 # of iterations
intervals = matrix(data = NA, nrow = n, ncol = 3)
bad_intervals = matrix(data = NA, nrow = n, ncol = 3) # showing what happens if you assume the wrong distribution
results = c()
bad_results = c()

for (i in 1:n){
  our_sample = sample(population,size = 65,replace = FALSE)
  intervals[i,1] = nptol.int(our_sample,alpha = .10,P = .90,side = 2,method = &quot;WILKS&quot;)[3]$&#39;2-sided.lower&#39;
  intervals[i,2] = nptol.int(our_sample,alpha = .10,P = .90,side = 2,method = &quot;WILKS&quot;)[4]$&#39;2-sided.upper&#39;
  intervals[i,3] = length(population[population &lt;= intervals[i,2]])/1000 - length(population[population &lt;= intervals[i,1]])/1000
  
  bad_intervals[i,1] = normtol.int(x = our_sample,alpha = .10,P = .9,side = 2,method = &quot;HE&quot;)$&#39;2-sided.lower&#39;
  bad_intervals[i,2] = normtol.int(x = our_sample,alpha = .10,P = .9,side = 2,method = &quot;HE&quot;)$&#39;2-sided.upper&#39;
  bad_intervals[i,3] = length(population[population &lt;= bad_intervals[i,2]])/1000 - length(population[population &lt;= bad_intervals[i,1]])/1000
  
  if(intervals[i,3] &lt; .90){
    results[i] = FALSE
  } else (results[i] = TRUE)
  
  if(bad_intervals[i,3] &lt; .90){
    bad_results[i] = FALSE
  } else (bad_results[i] = TRUE)
}

#intervals
sum(results)/n # confidence level</code></pre>
<pre><code>## [1] 0.887</code></pre>
<pre class="r"><code>sum(bad_results)/n # confidence level</code></pre>
<pre><code>## [1] 0.8</code></pre>
<p><strong>Example Application</strong></p>
<p>Turning towards a real-world example of applying Wilk’s method by computing a nonparametric interval using one of the most popular built-in R data sets - mtcars, we showcase how simple it is to do so using the tolerance package as used in part of the simulation. Specifically, we computed a 95/95 tolerance interval for the weight of automobiles based on the 32 automobiles in the data set, assuming them to be an independently drawn subset of a larger population of automobiles. We also plotted the interval limits on top of the data histogram generated using the plottol function included with the tolerance package.</p>
<pre class="r"><code>result = tolerance::nptol.int(mtcars$wt,alpha = .05,P = .95,side = 2,method = &quot;WILKS&quot;) # 90/90 nonparametric tolerance
tolerance::plottol(tol.out = result,x = mtcars$wt,plot.type = &quot;hist&quot;,x.lab = &quot;&quot;)</code></pre>
<p><img src="/professional/Nonparametric%20Tolerance%20Intervals_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p><strong>Results and Conclusion</strong>
Wilk’s method for nonparametric tolerance intervals is a powerful tool when you don’t want to assume a specific distribution for your data. Unlike parametric methods, which require strong assumptions, this approach lets you make robust inferences with minimal constraints. The trade-off? You often need a larger sample size to get the same level of precision as parametric alternatives.</p>
<p>So when should you use it? If you’re working with messy real-world data—like quality control measurements, environmental studies, or reliability testing—where normality is questionable, Wilk’s method can be a lifesaver. It’s simple, distribution-free, and gets the job done.</p>
<p>That said, nonparametric approaches aren’t always the most efficient choice. If you’re confident in your data’s distribution, parametric methods might give you tighter intervals with less data. But if you want flexibility and fewer assumptions, this is a solid way to go.</p>
<p>The <em>tolerance</em> package is a solid tool for calculating tolerance intervals in a variety of scenarios—this post just scratches the surface. If you’re curious, check out the documentation on CRAN. Got any thoughts, critiques, or better approaches? I’m always up for a discussion!</p>
<p><strong>Just the code, please</strong></p>
<p><strong>References</strong></p>
<ol style="list-style-type: decimal">
<li>tolerance: An R Package for Estimating Tolerance Intervals</li>
<li>Wilk’s Formula Applied to Computational Tools: A Practical
Discussion and Verification</li>
<li><a href="https://en.wikipedia.org/wiki/Tolerance_interval" class="uri">https://en.wikipedia.org/wiki/Tolerance_interval</a></li>
<li><a href="https://en.wikipedia.org/wiki/Coverage_probability" class="uri">https://en.wikipedia.org/wiki/Coverage_probability</a></li>
</ol>
